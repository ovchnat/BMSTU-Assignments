{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45db0dbb",
   "metadata": {},
   "source": [
    "# ДОМАШНЕЕ ЗАДАНИЕ 3. Классификация текстовых документов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8c511",
   "metadata": {},
   "source": [
    "Папулин С.Ю. (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143c109",
   "metadata": {},
   "source": [
    "## Цель работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f81848",
   "metadata": {},
   "source": [
    "Приобрести опыт решения практических задач по машинному обучению, таких как анализ и визуализация исходных данных, обучение, выбор и оценка качества моделей предсказания, посредством языка программирования Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13911917",
   "metadata": {},
   "source": [
    "## Вариант 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae8847",
   "metadata": {},
   "source": [
    "Набор рецензий на фильмы (reviews)\n",
    "Файл: data/reviews.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8f8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваш вариант -  3\n"
     ]
    }
   ],
   "source": [
    "surname = \"Овчинникова\" # Ваша фамилия\n",
    "\n",
    "alp = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "w = [4, 42, 21, 21, 34,  1, 44, 26, 18, 43, 38, 26, 18, 43,  3, 49, 45,\n",
    "        7, 42, 25,  4,  9, 36, 33, 31, 29,  5, 31,  4, 19, 24, 27, 33]\n",
    "d = dict(zip(alp, w))\n",
    "variant =  sum([d[el] for el in surname.lower()]) % 3 + 1\n",
    "print(\"Ваш вариант - \", variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bc34fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf64f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalRS = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ba0ec",
   "metadata": {},
   "source": [
    "## Задание 1. Оценка качества классификации текстовых данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03cc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d879b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068724fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../lib/\")\n",
    "\n",
    "from plot_confusion_matrix import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc0deb",
   "metadata": {},
   "source": [
    "## Загрузка исходных данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419762d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/reviews.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62aca951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No header for reviews table\n",
    "reviews_ds = pd.read_csv(FILE_PATH, sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b316900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>unless bob crane is someone of particular inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>finds a way to tell a simple story , perhaps t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ill-considered , unholy hokum .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>nijinsky says , 'i know how to suffer' and if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>the auteur's ear for the way fears and slights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>0</td>\n",
       "      <td>it's mildly sentimental , unabashedly consumer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>0</td>\n",
       "      <td>so verbally flatfooted and so emotionally pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>0</td>\n",
       "      <td>alternative medicine obviously has its merits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>0</td>\n",
       "      <td>a by-the-numbers patient/doctor pic that cover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>0</td>\n",
       "      <td>according to the script , grant and bullock's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "0      0  unless bob crane is someone of particular inte...\n",
       "1      1  finds a way to tell a simple story , perhaps t...\n",
       "2      0                   ill-considered , unholy hokum . \n",
       "3      0  nijinsky says , 'i know how to suffer' and if ...\n",
       "4      1  the auteur's ear for the way fears and slights...\n",
       "...   ..                                                ...\n",
       "10657  0  it's mildly sentimental , unabashedly consumer...\n",
       "10658  0  so verbally flatfooted and so emotionally pred...\n",
       "10659  0  alternative medicine obviously has its merits ...\n",
       "10660  0  a by-the-numbers patient/doctor pic that cover...\n",
       "10661  0  according to the script , grant and bullock's ...\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f292b3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>unless bob crane is someone of particular inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recommend</td>\n",
       "      <td>finds a way to tell a simple story , perhaps t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>ill-considered , unholy hokum .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>nijinsky says , 'i know how to suffer' and if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recommend</td>\n",
       "      <td>the auteur's ear for the way fears and slights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>it's mildly sentimental , unabashedly consumer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>so verbally flatfooted and so emotionally pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>alternative medicine obviously has its merits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>a by-the-numbers patient/doctor pic that cover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>not recommend</td>\n",
       "      <td>according to the script , grant and bullock's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                                                  1\n",
       "0      not recommend  unless bob crane is someone of particular inte...\n",
       "1          recommend  finds a way to tell a simple story , perhaps t...\n",
       "2      not recommend                   ill-considered , unholy hokum . \n",
       "3      not recommend  nijinsky says , 'i know how to suffer' and if ...\n",
       "4          recommend  the auteur's ear for the way fears and slights...\n",
       "...              ...                                                ...\n",
       "10657  not recommend  it's mildly sentimental , unabashedly consumer...\n",
       "10658  not recommend  so verbally flatfooted and so emotionally pred...\n",
       "10659  not recommend  alternative medicine obviously has its merits ...\n",
       "10660  not recommend  a by-the-numbers patient/doctor pic that cover...\n",
       "10661  not recommend  according to the script , grant and bullock's ...\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ds[0] = [\"recommend\" if r == 1 else \"not recommend\" for r in reviews_ds[0]]\n",
    "reviews_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2355eab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not recommend', 'recommend', 'not recommend', ...,\n",
       "       'not recommend', 'not recommend', 'not recommend'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the positive/negative information\n",
    "recomms = np.asarray(reviews_ds[0])\n",
    "recomms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a0a108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"unless bob crane is someone of particular interest to you , this film's impressive performances and adept direction aren't likely to leave a lasting impression . \",\n",
       "       'finds a way to tell a simple story , perhaps the simplest story of all , in a way that seems compelling and even original . ',\n",
       "       'ill-considered , unholy hokum . ', ...,\n",
       "       'alternative medicine obviously has its merits . . . but ayurveda does the field no favors . ',\n",
       "       'a by-the-numbers patient/doctor pic that covers all the usual ground',\n",
       "       \"according to the script , grant and bullock's characters are made for each other . but you'd never guess that from the performances . \"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the reviews\n",
    "reviews = reviews_ds[1]\n",
    "reviews = np.asarray(reviews)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd86bd",
   "metadata": {},
   "source": [
    "## Разбиение загруженных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1adf9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the test/train\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews, recomms, test_size=0.2, random_state = globalRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3684e9",
   "metadata": {},
   "source": [
    "##  Перевод текстовых данных в векторный вид. \n",
    "Для этого воспользуйтесь средствами sklearn для трансформации текстовых документов в векторы TF-IDF (настроить на обучающем подмножестве, n-gram=1, слова в нижний регистр). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f816d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "508050d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range = (1,1), \n",
    "                                   stop_words = None, lowercase = True,\n",
    "                                   binary = False, strip_accents = None)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3a4cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_v = count_vectorizer.fit_transform(x_train)\n",
    "test_count_v = count_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c0a28f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(smooth_idf=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=None, \n",
    "                             use_idf=True, ngram_range=(1,1),\n",
    "                             smooth_idf=False)                        \n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46bbcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_v = vectorizer.fit_transform(x_train)\n",
    "tfidf_test_v = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e0e426c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8529x16494 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 143664 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca292e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed for this\n",
    "# tfidf_transformer = TfidfTransformer(norm=None, use_idf=True, smooth_idf=False)\n",
    "# tfidf_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6206f",
   "metadata": {},
   "source": [
    "## Построение K-ближайших соседей ($n=5$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032ca7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8183f87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_rev = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_rev.fit(tfidf_train_v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067eac8a",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fe6f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c83cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['recommend', 'not recommend', 'not recommend', ..., 'recommend',\n",
       "       'recommend', 'not recommend'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31c2db18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=500, random_state=12345)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В замечании к заданию random_state = 12345\n",
    "lin_rev = LogisticRegression(penalty = \"l2\", fit_intercept = True, max_iter = 500, C = 1,\n",
    "                                solver = \"lbfgs\", random_state = 12345)\n",
    "lin_rev.fit(tfidf_train_v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d6610",
   "metadata": {},
   "source": [
    "## Наивный Байес: модель Бернулли ($\\alpha=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f785db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87531bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1, binarize=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_rev = BernoulliNB(alpha = 1, binarize = None)\n",
    "bnb_rev.fit(train_count_v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366dbbc0",
   "metadata": {},
   "source": [
    "## Наивный Байес: полиномиальная модель ($\\alpha=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2df8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14d99984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_rev = MultinomialNB(alpha = 1)\n",
    "mnb_rev.fit(tfidf_train_v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f45874",
   "metadata": {},
   "source": [
    "## Определение качества классификации на тестовом подмножестве \n",
    "### Balanced-Accuracy, R, P, F1 for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69a9f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a2dad57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced-Accuracy:  0.726 \n",
      " Recall:  0.705 \n",
      " Precision:  0.735 \n",
      " F1 Score:  0.72 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_rev.predict(tfidf_test_v)\n",
    "knn_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "knn_recall = recall_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "knn_prec = precision_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "knn_f1 = f1_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "\n",
    "print('Balanced-Accuracy: ', round(knn_bal_acc, 3), '\\n',  \n",
    "      'Recall: ', round(knn_recall, 3), '\\n',\n",
    "      'Precision: ', round(knn_prec, 3), '\\n',\n",
    "      'F1 Score: ', round(knn_f1, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545dfa9",
   "metadata": {},
   "source": [
    "### Balanced-Accuracy, R, P, F1 for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94f6fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced-Accuracy:  0.77 \n",
      " Recall:  0.747 \n",
      " Precision:  0.782 \n",
      " F1 Score:  0.764 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lin_rev.predict(tfidf_test_v)\n",
    "lin_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "lin_recall = recall_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "lin_prec = precision_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "lin_f1 = f1_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "\n",
    "print('Balanced-Accuracy: ', round(lin_bal_acc, 3), '\\n',  \n",
    "      'Recall: ', round(lin_recall, 3), '\\n',\n",
    "      'Precision: ', round(lin_prec, 3), '\\n',\n",
    "      'F1 Score: ', round(lin_f1, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bab546",
   "metadata": {},
   "source": [
    "### Balanced-Accuracy, R, P, F1 for Bernoulli Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17ab3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced-Accuracy:  0.771 \n",
      " Recall:  0.836 \n",
      " Precision:  0.739 \n",
      " F1 Score:  0.784 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = bnb_rev.predict(tfidf_test_v)\n",
    "bnb_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "bnb_recall = recall_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "bnb_prec = precision_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "bnb_f1 = f1_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "\n",
    "print('Balanced-Accuracy: ', round(bnb_bal_acc, 3), '\\n',  \n",
    "      'Recall: ', round(bnb_recall, 3), '\\n',\n",
    "      'Precision: ', round(bnb_prec, 3), '\\n',\n",
    "      'F1 Score: ', round(bnb_f1, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6198bc8",
   "metadata": {},
   "source": [
    "### Balanced-Accuracy, R, P, F1 for Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74edeccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced-Accuracy:  0.77 \n",
      " Recall:  0.747 \n",
      " Precision:  0.782 \n",
      " F1 Score:  0.764 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lin_rev.predict(tfidf_test_v)\n",
    "mnb_bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "mnb_recall = recall_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "mnb_prec = precision_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "mnb_f1 = f1_score(y_test, y_pred, pos_label = 'not recommend')\n",
    "\n",
    "print('Balanced-Accuracy: ', round(mnb_bal_acc, 3), '\\n',  \n",
    "      'Recall: ', round(mnb_recall, 3), '\\n',\n",
    "      'Precision: ', round(mnb_prec, 3), '\\n',\n",
    "      'F1 Score: ', round(mnb_f1, 3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba73867",
   "metadata": {},
   "source": [
    "## Определение времени обучения и предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39130bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train: \n",
      "  t = 0.006508213571428639s\n",
      "KNN Predict: \n",
      "  t = 0.6754368571428456s\n"
     ]
    }
   ],
   "source": [
    "knn_train_time = %timeit -qo knn_rev.fit(tfidf_train_v, y_train)\n",
    "print('KNN Train:', '\\n', f\" t = {knn_train_time.average}s\")\n",
    "\n",
    "knn_predict_time = %timeit -qo knn_rev.predict(tfidf_test_v)\n",
    "print('KNN Predict:', '\\n', f\" t = {knn_predict_time.average}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6cf012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train: \n",
      "  t = 0.006838332428571415s\n",
      "KNN Predict: \n",
      "  t = 0.6736828142857202s\n"
     ]
    }
   ],
   "source": [
    "knn_train_time = %timeit -qo  knn_rev.fit(tfidf_train_v, y_train)\n",
    "print('KNN Train:', '\\n', f\" t = {knn_train_time.average}s\")\n",
    "\n",
    "knn_predict_time = %timeit -qo knn_rev.predict(tfidf_test_v)\n",
    "print('KNN Predict:', '\\n', f\" t = {knn_predict_time.average}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f13c718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Train: \n",
      "  t = 0.10821485428571447s\n",
      "LR Predict: \n",
      "  t = 0.00016051872857141398s\n"
     ]
    }
   ],
   "source": [
    "lr_train_time = %timeit -qo  lin_rev.fit(tfidf_train_v, y_train)\n",
    "print('LR Train:', '\\n', f\" t = {lr_train_time.average}s\")\n",
    "\n",
    "lr_predict_time = %timeit -qo lin_rev.predict(tfidf_test_v)\n",
    "print('LR Predict:', '\\n', f\" t = {lr_predict_time.average}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc323bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train: \n",
      "  t = 0.017761605428571393s\n",
      "KNN Predict: \n",
      "  t = 0.0008880559428571522s\n"
     ]
    }
   ],
   "source": [
    "bnb_train_time = %timeit -qo  bnb_rev.fit(tfidf_train_v, y_train)\n",
    "print('KNN Train:', '\\n', f\" t = {bnb_train_time.average}s\")\n",
    "\n",
    "bnb_predict_time = %timeit -qo bnb_rev.predict(tfidf_test_v)\n",
    "print('KNN Predict:', '\\n', f\" t = {bnb_predict_time.average}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "619966a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train: \n",
      "  t = 0.01759262385714286s\n",
      "KNN Predict: \n",
      "  t = 0.00040052051428571234s\n"
     ]
    }
   ],
   "source": [
    "mnb_train_time = %timeit -qo  mnb_rev.fit(tfidf_train_v, y_train)\n",
    "print('KNN Train:', '\\n', f\" t = {mnb_train_time.average}s\")\n",
    "\n",
    "mnb_predict_time = %timeit -qo mnb_rev.predict(tfidf_test_v)\n",
    "print('KNN Predict:', '\\n', f\" t = {mnb_predict_time.average}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228b832",
   "metadata": {},
   "source": [
    "## Значения в датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "272f874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Predict time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.725710</td>\n",
       "      <td>0.705164</td>\n",
       "      <td>0.734834</td>\n",
       "      <td>0.719693</td>\n",
       "      <td>6.84 ms ± 220 µs per loop (mean ± std. dev. of...</td>\n",
       "      <td>674 ms ± 38.9 ms per loop (mean ± std. dev. of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.769776</td>\n",
       "      <td>0.747418</td>\n",
       "      <td>0.781925</td>\n",
       "      <td>0.764282</td>\n",
       "      <td>108 ms ± 7.81 ms per loop (mean ± std. dev. of...</td>\n",
       "      <td>161 µs ± 14.2 µs per loop (mean ± std. dev. of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli</th>\n",
       "      <td>0.770837</td>\n",
       "      <td>0.835681</td>\n",
       "      <td>0.739203</td>\n",
       "      <td>0.784487</td>\n",
       "      <td>17.8 ms ± 728 µs per loop (mean ± std. dev. of...</td>\n",
       "      <td>888 µs ± 38 µs per loop (mean ± std. dev. of 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial Bernulli</th>\n",
       "      <td>0.769776</td>\n",
       "      <td>0.747418</td>\n",
       "      <td>0.781925</td>\n",
       "      <td>0.764282</td>\n",
       "      <td>17.6 ms ± 381 µs per loop (mean ± std. dev. of...</td>\n",
       "      <td>401 µs ± 26.4 µs per loop (mean ± std. dev. of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Balanced Accuracy    Recall  Precision  F1 Score  \\\n",
       "KNN                            0.725710  0.705164   0.734834  0.719693   \n",
       "Logistic Regression            0.769776  0.747418   0.781925  0.764282   \n",
       "Bernoulli                      0.770837  0.835681   0.739203  0.784487   \n",
       "Multinomial Bernulli           0.769776  0.747418   0.781925  0.764282   \n",
       "\n",
       "                                                             Train time  \\\n",
       "KNN                   6.84 ms ± 220 µs per loop (mean ± std. dev. of...   \n",
       "Logistic Regression   108 ms ± 7.81 ms per loop (mean ± std. dev. of...   \n",
       "Bernoulli             17.8 ms ± 728 µs per loop (mean ± std. dev. of...   \n",
       "Multinomial Bernulli  17.6 ms ± 381 µs per loop (mean ± std. dev. of...   \n",
       "\n",
       "                                                           Predict time  \n",
       "KNN                   674 ms ± 38.9 ms per loop (mean ± std. dev. of...  \n",
       "Logistic Regression   161 µs ± 14.2 µs per loop (mean ± std. dev. of...  \n",
       "Bernoulli             888 µs ± 38 µs per loop (mean ± std. dev. of 7...  \n",
       "Multinomial Bernulli  401 µs ± 26.4 µs per loop (mean ± std. dev. of...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [\n",
    "            (knn_bal_acc, knn_recall, knn_prec, knn_f1, knn_train_time, knn_predict_time),\n",
    "            (lin_bal_acc, lin_recall, lin_prec, lin_f1, lr_train_time, lr_predict_time),\n",
    "            (bnb_bal_acc, bnb_recall, bnb_prec, bnb_f1, bnb_train_time, bnb_predict_time),\n",
    "            (mnb_bal_acc, mnb_recall, mnb_prec, mnb_f1, mnb_train_time, mnb_predict_time),\n",
    "          ]\n",
    "\n",
    "models = ['KNN', 'Logistic Regression', 'Bernoulli', 'Multinomial Bernulli']\n",
    "\n",
    "metr_names = ['Balanced Accuracy', 'Recall', 'Precision', 'F1 Score', 'Train time', 'Predict time']\n",
    "\n",
    "df = pd.DataFrame.from_records(metrics, columns = metr_names, index = models)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2d6d0",
   "metadata": {},
   "source": [
    "## Задание 2. Оценка качества классификации текстовых данных посредством кросс-валидации (2 балла)\n",
    "Повторите решение первого задания с использованием стратифицированной кросс-валидации k-folds (k=4) для разделения исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9602a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random_state is needed for shuffle option, left it out\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "582c3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_mod(model_name, X = reviews, y = recomms, \n",
    "                trans_type = 'tfidf', _pos_label = 'not recommend'):\n",
    "    \n",
    "    _accs = []\n",
    "    _precs = []\n",
    "    _recalls = []\n",
    "    _f1s = []\n",
    "    _train_times = []\n",
    "    _predict_times = []\n",
    "    \n",
    "    for i_train, i_test in kf.split(X, y):\n",
    "        \n",
    "        x_train = [X[i] for i in i_train]\n",
    "        x_test = [X[i] for i in i_test]\n",
    "        \n",
    "        y_train = [y[i] for i in i_train]\n",
    "        y_test = [y[i] for i in i_test]\n",
    "    \n",
    "        if trans_type == 'tfidf':\n",
    "            v_train = vectorizer.fit_transform(x_train)\n",
    "            v_test = vectorizer.transform(x_test)\n",
    "        else :\n",
    "            v_train = count_vectorizer.fit_transform(x_train)\n",
    "            v_test  = count_vectorizer.transform(x_test)  \n",
    "\n",
    "        model_name.fit(v_train, y_train)\n",
    "        \n",
    "        # Measuring the quality by Balanced-Accuracy, R, P, F1\n",
    "        \n",
    "        y_pred = model_name.predict(v_test)\n",
    "        k_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "        k_rec = recall_score(y_test, y_pred, pos_label = _pos_label)\n",
    "        k_prec = precision_score(y_test, y_pred, pos_label = _pos_label)\n",
    "        k_f1 = f1_score(y_test, y_pred, pos_label = _pos_label)        \n",
    "        \n",
    "        # Calculating the time\n",
    "        \n",
    "        ttk = %timeit -qo model_name.fit(v_train, y_train)\n",
    "        ptk = %timeit -qo model_name.predict(v_test)\n",
    "        \n",
    "        ttk = ttk.average\n",
    "        ptk = ptk.average\n",
    "        \n",
    "        # Filling up the lists to find the average + finding the average itself \n",
    "        _accs.append(k_acc)\n",
    "        _recalls.append(k_rec)\n",
    "        _precs.append(k_prec)\n",
    "        _f1s.append(k_f1)\n",
    "        _train_times.append(ttk)\n",
    "        _predict_times.append(ptk)\n",
    "        \n",
    "        acc = sum(_accs) / len(_accs)\n",
    "        rec = sum(_recalls) / len(_recalls)\n",
    "        prec = sum(_precs) / len(_precs)\n",
    "        f1 = sum(_f1s) / len(_f1s)\n",
    "        tt = sum(_train_times) / len(_train_times)\n",
    "        pt = sum(_predict_times) / len(_predict_times)\n",
    "\n",
    "        \n",
    "    print('For', model_name, ':', '\\n')\n",
    "    print('Balanced Accuracy: ', round(acc, 4) )\n",
    "    print('Recall: ', round(rec, 4) )\n",
    "    print('Precision: ', round(prec, 4) ) \n",
    "    print('F1: ', round(f1, 4) )\n",
    "    print('TT: ', round(tt, 4) )\n",
    "    print('PT: ', round(pt, 4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f33d0d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For KNeighborsClassifier() : \n",
      "\n",
      "Balanced Accuracy:  0.7091\n",
      "Recall:  0.6909\n",
      "Precision:  0.7171\n",
      "F1:  0.7036\n",
      "TT:  0.0048\n",
      "PT:  0.8007\n"
     ]
    }
   ],
   "source": [
    "# Previously added classifications' names: count_vectorizer vectorizer knn_rev lin_rev bnb_rev mnb_rev\n",
    "kf_mod(knn_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bf7da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For LogisticRegression(C=1, random_state=12345) : \n",
      "\n",
      "Balanced Accuracy:  0.7591\n",
      "Recall:  0.7515\n",
      "Precision:  0.7631\n",
      "F1:  0.7572\n",
      "TT:  0.1196\n",
      "PT:  0.0002\n"
     ]
    }
   ],
   "source": [
    "kf_mod(lin_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "962411e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For BernoulliNB(alpha=1, binarize=None) : \n",
      "\n",
      "Balanced Accuracy:  0.7775\n",
      "Recall:  0.7892\n",
      "Precision:  0.7713\n",
      "F1:  0.7801\n",
      "TT:  0.0105\n",
      "PT:  0.001\n"
     ]
    }
   ],
   "source": [
    "kf_mod(bnb_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55a44d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MultinomialNB(alpha=1) : \n",
      "\n",
      "Balanced Accuracy:  0.7776\n",
      "Recall:  0.788\n",
      "Precision:  0.772\n",
      "F1:  0.7799\n",
      "TT:  0.0105\n",
      "PT:  0.0005\n"
     ]
    }
   ],
   "source": [
    "kf_mod(mnb_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c332cc",
   "metadata": {},
   "source": [
    "## Задание 3. Выбор модели (4 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43c0accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6b739db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomms_bl = [1 if r == \"recommend\" else 0 for r in recomms]\n",
    "recomms_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed408cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"unless bob crane is someone of particular interest to you , this film's impressive performances and adept direction aren't likely to leave a lasting impression . \",\n",
       "       'finds a way to tell a simple story , perhaps the simplest story of all , in a way that seems compelling and even original . ',\n",
       "       'ill-considered , unholy hokum . ', ...,\n",
       "       'alternative medicine obviously has its merits . . . but ayurveda does the field no favors . ',\n",
       "       'a by-the-numbers patient/doctor pic that covers all the usual ground',\n",
       "       \"according to the script , grant and bullock's characters are made for each other . but you'd never guess that from the performances . \"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = recomms_bl\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d01a70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews, recomms_bl, test_size=0.2, random_state = globalRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19e31d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e7e00",
   "metadata": {},
   "source": [
    "### Разбиение обучающего подмножества (train) посредством стратифицированной кросс-валидации с kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1a27c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "\tindices:\t[2122 2125 2126 ... 8526 8527 8528][   0    1    2 ... 2143 2144 2145]\n",
      "\ty:\t\t[0 0 0 ... 0 1 1][0 1 0 ... 0 1 0]\n",
      "Split 2\n",
      "\tindices:\t[   0    1    2 ... 8526 8527 8528][2122 2125 2126 ... 4262 4264 4265]\n",
      "\ty:\t\t[0 1 0 ... 0 1 1][0 0 0 ... 0 1 1]\n",
      "Split 3\n",
      "\tindices:\t[   0    1    2 ... 8526 8527 8528][4263 4266 4267 ... 6408 6413 6415]\n",
      "\ty:\t\t[0 1 0 ... 0 1 1][1 0 1 ... 0 0 1]\n",
      "Split 4\n",
      "\tindices:\t[   0    1    2 ... 6408 6413 6415][6374 6377 6380 ... 8526 8527 8528]\n",
      "\ty:\t\t[0 1 0 ... 0 0 1][1 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 4)\n",
    "\n",
    "splits = skf.split(X_train, y_train)\n",
    "i = 0\n",
    "for train_index, test_index in splits:\n",
    "    print(\"Split\", i + 1)\n",
    "    print(\"\\tindices:\\t{}{}\".format(train_index, test_index))\n",
    "    print(\"\\ty:\\t\\t{}{}\".format(y[train_index], y[test_index]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad712365",
   "metadata": {},
   "source": [
    "### Обучение и тестирование на разбитом обучающем подмножестве классификаторов с заданными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10fcf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество соседей: np.arange(1, 150, 20), параметр регуляризации: np.logspace(-2, 10, 8, base=10), \n",
    "# сглаживающий параметр: np.logspace(-4, 1, 8, base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a33e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr = np.arange(1, 150, 20)\n",
    "regul = np.logspace(-2, 10, 8, base = 10)\n",
    "par = np.logspace(-4, 1, 8, base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d278e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0facc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_knn ():\n",
    "    knn_mod = KNeighborsClassifier()\n",
    "    nbr = np.arange(1, 150, 20)\n",
    "    pipeline = Pipeline([\n",
    "        (\"TfIDF\", vectorizer), \n",
    "        (\"knn_model\", knn_mod)\n",
    "    ])\n",
    "\n",
    "    parameters = {\n",
    "        \"knn_model__n_neighbors\": nbr\n",
    "    }\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "    grid_class_parameters = {\n",
    "        \"estimator\": pipeline,\n",
    "        \"param_grid\": parameters,\n",
    "        \"cv\": skf\n",
    "    }\n",
    "    \n",
    "# Training\n",
    "    grid_search = GridSearchCV(**grid_class_parameters)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "# # Средние проверочные ошибки для каждой степени\n",
    "# mses_avg = np.abs(grid_search.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "# for indx, mse_avg in enumerate(mses_avg):\n",
    "#     print(\"{}) Test MSE for degree {}: {}\".format(indx+1, nbr[indx], mse_avg))\n",
    "\n",
    "# print(\"Best parameters:\", grid_search.best_params_[\"knn_model__n_neighbors\"])\n",
    "\n",
    "# # Предсказания для тестового множества\n",
    "# y_test__pred = grid_search.predict(X_test)\n",
    "\n",
    "# pipeline.get_params().keys()\n",
    "\n",
    "# mse_test = mean_squared_error(y_test, y_test__pred)\n",
    "\n",
    "# print(\"Test MSE:\", mse_test)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "# df = pd.DataFrame(grid_search.cv_results_)\n",
    "# df = df[['mean_fit_time', 'mean_score_time', 'mean_test_score', 'rank_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3fcd9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_LR():\n",
    "    regul = np.logspace(-2, 10, 8, base = 10)\n",
    "    pipeline = Pipeline([\n",
    "        (\"TfIDF\", vectorizer), \n",
    "        (\"linear_model\", lin_rev)\n",
    "    ])\n",
    "\n",
    "# pipeline.get_params().keys()\n",
    "\n",
    "    parameters = {\n",
    "        \"linear_model__C\": regul\n",
    "    }\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "    grid_class_parameters = {\n",
    "        \"estimator\": pipeline,\n",
    "        \"param_grid\": parameters,\n",
    "        \"cv\": skf\n",
    "    }\n",
    "\n",
    "    # Training\n",
    "    grid_search = GridSearchCV(**grid_class_parameters)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61be223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bnb():\n",
    "    \n",
    "    bnb_mod = BernoulliNB(binarize = None)\n",
    "\n",
    "    par = np.logspace(-4, 1, 8, base=10)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"TfIDF\", count_vectorizer), \n",
    "        (\"bernoulli_model\", bnb_mod)\n",
    "    ])\n",
    "\n",
    "    pipeline.get_params().keys()\n",
    "\n",
    "    parameters = {\n",
    "        \"bernoulli_model__alpha\": par\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "\n",
    "    grid_class_parameters = {\n",
    "        \"estimator\": pipeline,\n",
    "        \"param_grid\": parameters,\n",
    "        \"cv\": skf\n",
    "    }\n",
    "\n",
    "    # Training\n",
    "    grid_search = GridSearchCV(**grid_class_parameters)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62fb0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mnb():\n",
    "    \n",
    "    mnb_reg = MultinomialNB()\n",
    "\n",
    "    par = np.logspace(-4, 1, 8, base=10)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"TfIDF\", count_vectorizer), \n",
    "        (\"multinomial_model\", mnb_reg)\n",
    "    ])\n",
    "\n",
    "    pipeline.get_params().keys()\n",
    "\n",
    "    parameters = {\n",
    "        \"multinomial_model__alpha\": par\n",
    "    }\n",
    "\n",
    "    # Training parameters\n",
    "\n",
    "    grid_class_parameters = {\n",
    "        \"estimator\": pipeline,\n",
    "        \"param_grid\": parameters,\n",
    "        \"cv\": skf\n",
    "    }\n",
    "\n",
    "    # Training\n",
    "    grid_search = GridSearchCV(**grid_class_parameters)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc51b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn11 = calc_knn()\n",
    "gs_LR11 = calc_LR()\n",
    "gs_bnb11 = calc_bnb()\n",
    "gs_mnb11 = calc_mnb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e8dd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=None, \n",
    "                             use_idf=True, ngram_range=(2, 2),\n",
    "                             smooth_idf=False) \n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range = (2, 2), \n",
    "                                   stop_words = None, lowercase = True,\n",
    "                                   binary = False, strip_accents = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5992362",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn22 = calc_knn()\n",
    "gs_LR22 = calc_LR()\n",
    "gs_bnb22 = calc_bnb()\n",
    "gs_mnb22 = calc_mnb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e57d0bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=None, \n",
    "                             use_idf=True, ngram_range=(1, 2),\n",
    "                             smooth_idf=False) \n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range = (1, 2), \n",
    "                                   stop_words = None, lowercase = True,\n",
    "                                   binary = False, strip_accents = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4a64954",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn21 = calc_knn()\n",
    "gs_LR21 = calc_LR()\n",
    "gs_bnb21 = calc_bnb()\n",
    "gs_mnb21 = calc_mnb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9594ff23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Smoothing parameter')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axs[0, 0].plot(nbr, gs_knn11.cv_results_['mean_test_score'], color = 'magenta')\n",
    "axs[0, 0].plot(nbr, gs_knn22.cv_results_['mean_test_score'], color = 'cyan')\n",
    "axs[0, 0].plot(nbr, gs_knn21.cv_results_['mean_test_score'], color = 'red')\n",
    "axs[0, 0].set_title(\"KNN\")\n",
    "axs[0, 0].set_ylabel(\"Mean Score\")\n",
    "axs[0, 0].set_xlabel(\"Neighbours\")\n",
    "axs[1, 0].plot(regul, gs_LR11.cv_results_['mean_test_score'], color = 'magenta')\n",
    "axs[1, 0].plot(regul, gs_LR22.cv_results_['mean_test_score'], color = 'cyan')\n",
    "axs[1, 0].plot(regul, gs_LR21.cv_results_['mean_test_score'], color = 'red')\n",
    "axs[1, 0].set_title(\"Logistic Regression\")\n",
    "axs[1, 0].set_ylabel(\"Mean Score\")\n",
    "axs[1, 0].set_xlabel(\"Regularization Parameter\")\n",
    "axs[0, 1].plot(par, gs_bnb11.cv_results_['mean_test_score'], color = 'magenta')\n",
    "axs[0, 1].plot(par, gs_bnb22.cv_results_['mean_test_score'], color = 'cyan')\n",
    "axs[0, 1].plot(par, gs_bnb21.cv_results_['mean_test_score'], color = 'red')\n",
    "axs[0, 1].set_title(\"Bernoulli\")\n",
    "axs[0, 1].set_xscale('log')\n",
    "axs[0, 1].set_ylabel(\"Mean Score\")\n",
    "axs[0, 1].set_xlabel(\"Smoothing parameter\")\n",
    "axs[1, 1].plot(par, gs_mnb11.cv_results_['mean_test_score'], color = 'magenta')\n",
    "axs[1, 1].plot(par, gs_mnb22.cv_results_['mean_test_score'], color = 'cyan')\n",
    "axs[1, 1].plot(par, gs_mnb21.cv_results_['mean_test_score'], color = 'red')\n",
    "axs[1, 1].set_title(\"Multinomial\")\n",
    "axs[1, 1].set_ylabel(\"Mean Score\")\n",
    "axs[1, 1].set_xscale('log')\n",
    "axs[1, 1].set_xlabel(\"Smoothing parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0647021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14092314, 0.15630054, 0.15113807, 0.13778365, 0.13850081,\n",
       "        0.12571788, 0.12579292, 0.14908761]),\n",
       " 'std_fit_time': array([0.01106385, 0.01097965, 0.01610229, 0.0096376 , 0.01271927,\n",
       "        0.00019997, 0.00042198, 0.00768678]),\n",
       " 'mean_score_time': array([0.50466782, 0.6182189 , 0.60546911, 0.60244381, 0.61122805,\n",
       "        0.59639007, 0.62803197, 0.67901742]),\n",
       " 'std_score_time': array([0.01762967, 0.01495541, 0.00436012, 0.01809376, 0.0269524 ,\n",
       "        0.01164257, 0.0331433 , 0.01310923]),\n",
       " 'param_knn_model__n_neighbors': masked_array(data=[1, 21, 41, 61, 81, 101, 121, 141],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'knn_model__n_neighbors': 1},\n",
       "  {'knn_model__n_neighbors': 21},\n",
       "  {'knn_model__n_neighbors': 41},\n",
       "  {'knn_model__n_neighbors': 61},\n",
       "  {'knn_model__n_neighbors': 81},\n",
       "  {'knn_model__n_neighbors': 101},\n",
       "  {'knn_model__n_neighbors': 121},\n",
       "  {'knn_model__n_neighbors': 141}],\n",
       " 'split0_test_score': array([0.6511955 , 0.72480075, 0.73323957, 0.74683544, 0.75058603,\n",
       "        0.75246132, 0.73933427, 0.74496015]),\n",
       " 'split1_test_score': array([0.64915572, 0.72138837, 0.72185741, 0.72467167, 0.72091932,\n",
       "        0.72091932, 0.71013133, 0.71247655]),\n",
       " 'split2_test_score': array([0.66275797, 0.72701689, 0.73874296, 0.73545966, 0.73076923,\n",
       "        0.73405253, 0.73217636, 0.72232645]),\n",
       " 'split3_test_score': array([0.67964353, 0.73874296, 0.75703565, 0.76454034, 0.75375235,\n",
       "        0.74624765, 0.7532833 , 0.74155722]),\n",
       " 'mean_test_score': array([0.66068818, 0.72798724, 0.7377189 , 0.74287678, 0.73900673,\n",
       "        0.73842021, 0.73373132, 0.73033009]),\n",
       " 'std_test_score': array([0.01211092, 0.00652545, 0.01270666, 0.01475992, 0.0136613 ,\n",
       "        0.01208076, 0.01559688, 0.01344335]),\n",
       " 'rank_test_score': array([8, 7, 4, 1, 2, 3, 5, 6])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_knn11.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d12dc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time counter by best ranking test score\n",
    "\n",
    "gs_knn11_tt = float(gs_knn11.cv_results_['mean_fit_time'][np.where(gs_knn11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_knn22_tt = float(gs_knn22.cv_results_['mean_fit_time'][np.where(gs_knn22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_knn21_tt = float(gs_knn21.cv_results_['mean_fit_time'][np.where(gs_knn21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_LR11_tt = float(gs_LR11.cv_results_['mean_fit_time'][np.where(gs_LR11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_LR22_tt = float(gs_LR22.cv_results_['mean_fit_time'][np.where(gs_LR22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_LR21_tt = float(gs_LR21.cv_results_['mean_fit_time'][np.where(gs_LR21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_bnb11_tt = float(gs_bnb11.cv_results_['mean_fit_time'][np.where(gs_bnb11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_bnb22_tt = float(gs_bnb22.cv_results_['mean_fit_time'][np.where(gs_bnb22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_bnb21_tt = float(gs_bnb21.cv_results_['mean_fit_time'][np.where(gs_bnb21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_mnb11_tt = float(gs_mnb11.cv_results_['mean_fit_time'][np.where(gs_mnb11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_mnb22_tt = float(gs_mnb22.cv_results_['mean_fit_time'][np.where(gs_knn22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_mnb21_tt = float(gs_mnb21.cv_results_['mean_fit_time'][np.where(gs_knn21.cv_results_['rank_test_score'] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "48dfc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict time counter on test\n",
    "\n",
    "gs_knn11_pt = %timeit -qo gs_knn11.best_estimator_.predict(x_test)\n",
    "gs_knn22_pt = %timeit -qo gs_knn22.best_estimator_.predict(x_test)\n",
    "gs_knn21_pt = %timeit -qo gs_knn21.best_estimator_.predict(x_test)\n",
    "\n",
    "gs_LR11_pt = %timeit -qo gs_LR11.best_estimator_.predict(x_test)\n",
    "gs_LR22_pt = %timeit -qo gs_LR22.best_estimator_.predict(x_test)\n",
    "gs_LR21_pt = %timeit -qo gs_LR21.best_estimator_.predict(x_test)\n",
    "\n",
    "gs_bnb11_pt = %timeit -qo gs_bnb11.best_estimator_.predict(x_test)\n",
    "gs_bnb22_pt = %timeit -qo gs_bnb22.best_estimator_.predict(x_test)\n",
    "gs_bnb21_pt = %timeit -qo gs_bnb21.best_estimator_.predict(x_test)\n",
    "\n",
    "gs_mnb11_pt = %timeit -qo gs_mnb11.best_estimator_.predict(x_test)\n",
    "gs_mnb22_pt = %timeit -qo gs_mnb22.best_estimator_.predict(x_test)\n",
    "gs_mnb21_pt = %timeit -qo gs_mnb21.best_estimator_.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "198b722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameter \n",
    "\n",
    "gs_knn11_best_par = int(nbr[np.where(gs_knn11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_knn22_best_par = int(nbr[np.where(gs_knn22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_knn21_best_par = int(nbr[np.where(gs_knn21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_LR11_best_par = int(nbr[np.where(gs_LR11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_LR22_best_par = int(nbr[np.where(gs_LR22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_LR21_best_par = int(nbr[np.where(gs_LR21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_bnb11_best_par = int(nbr[np.where(gs_bnb11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_bnb22_best_par = int(nbr[np.where(gs_bnb22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_bnb21_best_par = int(nbr[np.where(gs_bnb21.cv_results_['rank_test_score'] == 1)])\n",
    "\n",
    "gs_mnb11_best_par = int(nbr[np.where(gs_mnb11.cv_results_['rank_test_score'] == 1)])\n",
    "gs_mnb22_best_par = int(nbr[np.where(gs_mnb22.cv_results_['rank_test_score'] == 1)])\n",
    "gs_mnb21_best_par = int(nbr[np.where(gs_mnb21.cv_results_['rank_test_score'] == 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30f1c8",
   "metadata": {},
   "source": [
    "### Итоговые данные по всем методам для лучших моделей (метод, n-gram, значение параметра модели, время обучения, время предсказания)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5acc33fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>N Grams</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Predict time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>61</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>721 ms ± 23.1 ms per loop (mean ± std. dev. of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>2, 2</td>\n",
       "      <td>61</td>\n",
       "      <td>0.376497</td>\n",
       "      <td>518 ms ± 12.2 ms per loop (mean ± std. dev. of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>2, 1</td>\n",
       "      <td>61</td>\n",
       "      <td>0.376497</td>\n",
       "      <td>761 ms ± 36 ms per loop (mean ± std. dev. of 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.500010</td>\n",
       "      <td>32.9 ms ± 1.17 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>2, 2</td>\n",
       "      <td>41</td>\n",
       "      <td>3.272012</td>\n",
       "      <td>48.1 ms ± 2.44 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>2, 1</td>\n",
       "      <td>61</td>\n",
       "      <td>3.272012</td>\n",
       "      <td>71.1 ms ± 2.52 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>101</td>\n",
       "      <td>0.128411</td>\n",
       "      <td>35.1 ms ± 2.18 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>2, 2</td>\n",
       "      <td>121</td>\n",
       "      <td>0.373268</td>\n",
       "      <td>58.4 ms ± 2.77 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>2, 1</td>\n",
       "      <td>121</td>\n",
       "      <td>0.373268</td>\n",
       "      <td>73.1 ms ± 4.81 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Multinomial Bernulli</td>\n",
       "      <td>1, 1</td>\n",
       "      <td>121</td>\n",
       "      <td>0.126564</td>\n",
       "      <td>37.9 ms ± 3.43 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Multinomial Bernulli</td>\n",
       "      <td>2, 2</td>\n",
       "      <td>121</td>\n",
       "      <td>0.390185</td>\n",
       "      <td>60.8 ms ± 7.83 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Multinomial Bernulli</td>\n",
       "      <td>2, 1</td>\n",
       "      <td>121</td>\n",
       "      <td>0.390185</td>\n",
       "      <td>71.5 ms ± 4.46 ms per loop (mean ± std. dev. o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model N Grams  Parameter  Train time  \\\n",
       "0                    KNN    1, 1         61    0.137784   \n",
       "1                    KNN    2, 2         61    0.376497   \n",
       "2                    KNN    2, 1         61    0.376497   \n",
       "3    Logistic Regression    1, 1         41    0.500010   \n",
       "4    Logistic Regression    2, 2         41    3.272012   \n",
       "5    Logistic Regression    2, 1         61    3.272012   \n",
       "6              Bernoulli    1, 1        101    0.128411   \n",
       "7              Bernoulli    2, 2        121    0.373268   \n",
       "8              Bernoulli    2, 1        121    0.373268   \n",
       "9   Multinomial Bernulli    1, 1        121    0.126564   \n",
       "10  Multinomial Bernulli    2, 2        121    0.390185   \n",
       "11  Multinomial Bernulli    2, 1        121    0.390185   \n",
       "\n",
       "                                         Predict time  \n",
       "0   721 ms ± 23.1 ms per loop (mean ± std. dev. of...  \n",
       "1   518 ms ± 12.2 ms per loop (mean ± std. dev. of...  \n",
       "2   761 ms ± 36 ms per loop (mean ± std. dev. of 7...  \n",
       "3   32.9 ms ± 1.17 ms per loop (mean ± std. dev. o...  \n",
       "4   48.1 ms ± 2.44 ms per loop (mean ± std. dev. o...  \n",
       "5   71.1 ms ± 2.52 ms per loop (mean ± std. dev. o...  \n",
       "6   35.1 ms ± 2.18 ms per loop (mean ± std. dev. o...  \n",
       "7   58.4 ms ± 2.77 ms per loop (mean ± std. dev. o...  \n",
       "8   73.1 ms ± 4.81 ms per loop (mean ± std. dev. o...  \n",
       "9   37.9 ms ± 3.43 ms per loop (mean ± std. dev. o...  \n",
       "10  60.8 ms ± 7.83 ms per loop (mean ± std. dev. o...  \n",
       "11  71.5 ms ± 4.46 ms per loop (mean ± std. dev. o...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN \n",
    "\n",
    "n_grams_1 = \"1, 1\"\n",
    "n_grams_2 = \"2, 2\"\n",
    "n_grams_3 = \"2, 1\"\n",
    "knn_metrics = [\n",
    "                ('KNN', n_grams_1, gs_knn11_best_par, gs_knn11_tt, gs_knn11_pt),\n",
    "                ('KNN', n_grams_2, gs_knn22_best_par, gs_knn21_tt, gs_knn22_pt),\n",
    "                ('KNN', n_grams_3, gs_knn21_best_par, gs_knn21_tt, gs_knn21_pt),\n",
    "    \n",
    "                ('Logistic Regression', n_grams_1, gs_LR11_best_par, gs_LR11_tt, gs_LR11_pt),\n",
    "                ('Logistic Regression', n_grams_2, gs_LR22_best_par, gs_LR21_tt, gs_LR22_pt),\n",
    "                ('Logistic Regression', n_grams_3, gs_LR21_best_par, gs_LR21_tt, gs_LR21_pt),\n",
    "    \n",
    "                ('Bernoulli', n_grams_1, gs_bnb11_best_par, gs_bnb11_tt, gs_bnb11_pt),\n",
    "                ('Bernoulli', n_grams_2, gs_bnb22_best_par, gs_bnb21_tt, gs_bnb22_pt),\n",
    "                ('Bernoulli', n_grams_3, gs_bnb21_best_par, gs_bnb21_tt, gs_bnb21_pt),\n",
    "    \n",
    "                ('Multinomial Bernulli', n_grams_1, gs_mnb11_best_par, gs_mnb11_tt, gs_mnb11_pt),\n",
    "                ('Multinomial Bernulli', n_grams_2, gs_mnb22_best_par, gs_mnb21_tt, gs_mnb22_pt),\n",
    "                ('Multinomial Bernulli', n_grams_3, gs_mnb21_best_par, gs_mnb21_tt, gs_mnb21_pt),\n",
    "              ]\n",
    "\n",
    "metrics_labels = [\"Model\", \"N Grams\", \"Parameter\", \"Train time\", \"Predict time\"]\n",
    "\n",
    "df_f = pd.DataFrame.from_records(knn_metrics, columns = metrics_labels)\n",
    "df_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32404d",
   "metadata": {},
   "source": [
    "# Задание 4. Оценка влияния количества признаков FeatureHasher на качество классификации (2 баллов)\n",
    "Как будет меняться качество классификации для обозначенных ранее методов при использовании FeatureHasher (или HashingVectorizer) из пакета sklearn перед TF-IDF преобразованием?\n",
    "\n",
    "Количество признаков: np.logspace(1, 5, 5, base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "57daab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.logspace(1, 5, 5, base = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7315c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_v = HashingVectorizer(norm=None, alternate_sign = False,\n",
    "                                    lowercase = True, stop_words = None,\n",
    "                                    ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2c3ec769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_knn_hv():\n",
    "    knn_mod = KNeighborsClassifier()\n",
    "    pipeline = Pipeline([\n",
    "        (\"hash_v\", hash_v),\n",
    "        (\"TfIDF\", TfidfTransformer(use_idf=True, smooth_idf=False)), \n",
    "        (\"knn_model\", knn_mod)\n",
    "    ])\n",
    "    \n",
    "    gs_knn_hv = GridSearchCV(pipeline, {\"hash_v__n_features\": feats.astype(int)}, cv = 4)\n",
    "    gs_knn_hv.fit(x_train, y_train)\n",
    "    \n",
    "    return gs_knn_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d72fdbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.48938793 0.53605288 0.60687213        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs_knn = calc_knn_hv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "85be18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit_time = gs_knn.cv_results_['mean_fit_time'][np.where(gs_knn.cv_results_['rank_test_score'] == 1)][0]\n",
    "knn_pred_time = %timeit -qo gs_knn.best_estimator_.predict(x_test)\n",
    "knn_score = gs_knn.cv_results_['mean_test_score'][np.where(gs_knn.cv_results_['rank_test_score'] == 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "49576ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_LR_hv():\n",
    "    pipeline = Pipeline([\n",
    "        (\"hash_v\", hash_v),\n",
    "        (\"TfIDF\", TfidfTransformer(use_idf=True, smooth_idf=False)), \n",
    "        (\"LR_model\", lin_rev)\n",
    "    ])\n",
    "    \n",
    "    gs_LR_hv = GridSearchCV(pipeline, {\"hash_v__n_features\": feats.astype(int)}, cv = 4)\n",
    "    gs_LR_hv.fit(x_train, y_train)\n",
    "    \n",
    "    return gs_LR_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "11174ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.51565176 0.58447609 0.66725242        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs_LR_hv = calc_LR_hv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "342d44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_fit_time = gs_LR_hv.cv_results_['mean_fit_time'][np.where(gs_LR_hv.cv_results_['rank_test_score'] == 1)][0]\n",
    "LR_pred_time = %timeit -qo gs_LR_hv.best_estimator_.predict(x_test)\n",
    "LR_score = gs_LR_hv.cv_results_['mean_test_score'][np.where(gs_LR_hv.cv_results_['rank_test_score'] == 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "721083e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bnb_hv():\n",
    "    bnb_mod = BernoulliNB(binarize = None)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"hash_v\", hash_v),\n",
    "        (\"TfIDF\", TfidfTransformer(use_idf=True, smooth_idf=False)), \n",
    "        (\"Bernoulli_model\", bnb_mod)\n",
    "    ])\n",
    "    \n",
    "    gs_bnb_hv = GridSearchCV(pipeline, {\"hash_v__n_features\": feats.astype(int)}, cv = 4)\n",
    "    gs_bnb_hv.fit(x_train, y_train)\n",
    "    \n",
    "    return gs_bnb_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c5f196ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.51635576 0.58388979 0.66279706        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs_bnb_hv = calc_bnb_hv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "29a28c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_fit_time = gs_bnb_hv.cv_results_['mean_fit_time'][np.where(gs_bnb_hv.cv_results_['rank_test_score'] == 1)][0]\n",
    "bnb_pred_time = %timeit -qo gs_bnb_hv.best_estimator_.predict(x_test)\n",
    "bnb_score = gs_bnb_hv.cv_results_['mean_test_score'][np.where(gs_bnb_hv.cv_results_['rank_test_score'] == 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "667aaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mnb_hv():\n",
    "    \n",
    "    mnb_reg = MultinomialNB()\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        (\"hash_v\", hash_v),\n",
    "        (\"TfIDF\", TfidfTransformer(use_idf=True, smooth_idf=False)), \n",
    "        (\"Multinomial_model\", mnb_reg)\n",
    "    ])\n",
    "    \n",
    "    gs_mnb_hv = GridSearchCV(pipeline, {\"hash_v__n_features\": feats.astype(int)}, cv = 4)\n",
    "    gs_mnb_hv.fit(x_train, y_train)    \n",
    "    \n",
    "    return gs_mnb_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "561c4696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1450: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n",
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 120, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 618, in score\n",
      "    Xt = transform.transform(Xt)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1500, in transform\n",
      "    X = normalize(X, norm=self.norm, copy=False)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1904, in normalize\n",
      "    X = check_array(X, accept_sparse=sparse_format, copy=copy,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 650, in check_array\n",
      "    array = _ensure_sparse_format(array, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 448, in _ensure_sparse_format\n",
      "    _assert_all_finite(spmatrix.data,\n",
      "  File \"C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 103, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.51494885 0.58506245 0.66279706        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs_mnb_hv = calc_mnb_hv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a1ba208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_fit_time = gs_mnb_hv.cv_results_['mean_fit_time'][np.where(gs_mnb_hv.cv_results_['rank_test_score'] == 1)][0]\n",
    "mnb_pred_time = %timeit -qo gs_mnb_hv.best_estimator_.predict(x_test)\n",
    "mnb_score = gs_mnb_hv.cv_results_['mean_test_score'][np.where(gs_mnb_hv.cv_results_['rank_test_score'] == 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a9bbc94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.08442497, 0.08908737, 0.0847137 , 0.08703607, 0.09421897]),\n",
       " 'std_fit_time': array([0.0102591 , 0.00630516, 0.00641797, 0.00843135, 0.00099229]),\n",
       " 'mean_score_time': array([0.66319054, 0.68192399, 0.52775681, 0.02873617, 0.0304926 ]),\n",
       " 'std_score_time': array([0.01716725, 0.04637696, 0.00688731, 0.0083437 , 0.00676056]),\n",
       " 'param_hash_v__n_features': masked_array(data=[10, 100, 1000, 10000, 100000],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'hash_v__n_features': 10},\n",
       "  {'hash_v__n_features': 100},\n",
       "  {'hash_v__n_features': 1000},\n",
       "  {'hash_v__n_features': 10000},\n",
       "  {'hash_v__n_features': 100000}],\n",
       " 'split0_test_score': array([0.49976559, 0.54102203, 0.59446789,        nan,        nan]),\n",
       " 'split1_test_score': array([0.48405253, 0.53142589, 0.61116323,        nan,        nan]),\n",
       " 'split2_test_score': array([0.48874296, 0.52954972, 0.60131332,        nan,        nan]),\n",
       " 'split3_test_score': array([0.48499062, 0.54221388, 0.62054409,        nan,        nan]),\n",
       " 'mean_test_score': array([0.48938793, 0.53605288, 0.60687213,        nan,        nan]),\n",
       " 'std_test_score': array([0.00624329, 0.00562029, 0.0098755 ,        nan,        nan]),\n",
       " 'rank_test_score': array([3, 2, 1, 4, 5])}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gs_knn.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7ba8cee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train time</th>\n",
       "      <th>Predict time</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.090330</td>\n",
       "      <td>730 ms ± 40.5 ms per loop (mean ± std. dev. of...</td>\n",
       "      <td>0.606872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.118363</td>\n",
       "      <td>31 ms ± 1.18 ms per loop (mean ± std. dev. of ...</td>\n",
       "      <td>0.667252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli</th>\n",
       "      <td>0.096338</td>\n",
       "      <td>31.2 ms ± 1.55 ms per loop (mean ± std. dev. o...</td>\n",
       "      <td>0.662797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial Bernulli</th>\n",
       "      <td>0.082394</td>\n",
       "      <td>29.1 ms ± 1.17 ms per loop (mean ± std. dev. o...</td>\n",
       "      <td>0.662797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Train time  \\\n",
       "KNN                     0.090330   \n",
       "Logistic Regression     0.118363   \n",
       "Bernoulli               0.096338   \n",
       "Multinomial Bernulli    0.082394   \n",
       "\n",
       "                                                           Predict time  \\\n",
       "KNN                   730 ms ± 40.5 ms per loop (mean ± std. dev. of...   \n",
       "Logistic Regression   31 ms ± 1.18 ms per loop (mean ± std. dev. of ...   \n",
       "Bernoulli             31.2 ms ± 1.55 ms per loop (mean ± std. dev. o...   \n",
       "Multinomial Bernulli  29.1 ms ± 1.17 ms per loop (mean ± std. dev. o...   \n",
       "\n",
       "                      Balanced Accuracy  \n",
       "KNN                            0.606872  \n",
       "Logistic Regression            0.667252  \n",
       "Bernoulli                      0.662797  \n",
       "Multinomial Bernulli           0.662797  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [\n",
    "            (knn_fit_time, knn_pred_time, knn_score),\n",
    "            (LR_fit_time, LR_pred_time, LR_score),\n",
    "            (bnb_fit_time, bnb_pred_time, bnb_score),\n",
    "            (mnb_fit_time, mnb_pred_time, mnb_score)\n",
    "         ]\n",
    "\n",
    "metrics_labels = [\"Train time\", \"Predict time\", \"Balanced Accuracy\"]\n",
    "\n",
    "models = ['KNN', 'Logistic Regression', 'Bernoulli', 'Multinomial Bernulli']\n",
    "\n",
    "df_hv = pd.DataFrame.from_records(values, columns = metrics_labels, index = models)\n",
    "df_hv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
